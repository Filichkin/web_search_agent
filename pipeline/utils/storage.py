import json
from datetime import datetime
from typing import Any

from pipeline.utils.logging import logger


def save_search_results(
    query: str,
    results: Any,
    output_file: str = 'results.json',
    max_items: int = 5,
    already_enriched: bool = False
) -> int:
    now = datetime.now().isoformat(timespec='seconds')

    logger.info(
        'üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞: query={!r}, '
        '–º–∞–∫—Å. —ç–ª–µ–º–µ–Ω—Ç–æ–≤={}, —Ñ–∞–π–ª={!r}, enriched={}',
        query, max_items, output_file, already_enriched
    )

    # —á–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if not isinstance(data, list):
                logger.warning(
                    '–§–∞–π–ª {} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–∫–æ–º, –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º',
                    output_file
                    )
                data = []
    except FileNotFoundError:
        logger.info('–§–∞–π–ª {} –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π', output_file)
        data = []
    except json.JSONDecodeError:
        logger.warning(
            '–§–∞–π–ª {} –ø–æ–≤—Ä–µ–∂–¥—ë–Ω –∏–ª–∏ –ø—É—Å—Ç–æ–π, –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º',
            output_file
            )
        data = []

    # –º–Ω–æ–∂–µ—Å—Ç–≤–æ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–ª—é—á–µ–π –¥–ª—è –¥–µ–¥—É–ø–∞
    existing_urls = {
        (rec.get('url') or '').strip().lower()
        for rec in data if isinstance(rec, dict)
    }

    added = 0

    if already_enriched:
        # –æ–∂–∏–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π {'url','title','snippet'}
        if isinstance(results, list):
            logger.info('–ü–æ–ª—É—á–µ–Ω–æ {} enriched-—ç–ª–µ–º–µ–Ω—Ç–æ–≤, –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ {}',
                        len(results), min(len(results), max_items))
            for index, element in enumerate(results[:max_items], start=1):
                url = (element.get('url') or '').strip()
                url_key = url.lower()
                if not url or url_key in existing_urls:
                    logger.info(
                        '[{}] –ü—Ä–æ–ø—É—Å–∫ (–¥—É–±–ª–∏–∫–∞—Ç –∏–ª–∏ –ø—É—Å—Ç–æ–π URL): {!r}',
                        index,
                        url
                        )
                    continue
                title = (element.get('title') or '').strip() or url
                description = (element.get('snippet') or '').strip()
                data.append({
                    'query': query,
                    'date': now,
                    'title': title,
                    'description': description,
                    'url': url
                })
                existing_urls.add(url_key)
                added += 1
        else:
            logger.warning(
                '–û–∂–∏–¥–∞–ª—Å—è —Å–ø–∏—Å–æ–∫ enriched-—ç–ª–µ–º–µ–Ω—Ç–æ–≤, –ø–æ–ª—É—á–µ–Ω–æ: {!r}',
                type(results)
                )
    else:
        # —Å—ã—Ä–æ–π —Å–ø–∏—Å–æ–∫ (—Å—Ç—Ä–æ–∫–∏ JSON –∏–ª–∏ —Å–ª–æ–≤–∞—Ä–∏)
        if isinstance(results, list):
            logger.info('–ü–æ–ª—É—á–µ–Ω–æ {} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ {}',
                        len(results), min(len(results), max_items))
            for index, element in enumerate(results[:max_items], start=1):
                try:
                    data_el = (
                        json.loads(element) if isinstance(element, str)
                        else element
                        )
                except Exception as error:
                    logger.warning(
                        '[{}] –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —ç–ª–µ–º–µ–Ω—Ç: {}',
                        index,
                        error
                        )
                    continue
                url = (data_el.get('url') or '').strip()
                url_key = url.lower()
                if not url or url_key in existing_urls:
                    logger.info(
                        '[{}] –ü—Ä–æ–ø—É—Å–∫ (–¥—É–±–ª–∏–∫–∞—Ç –∏–ª–∏ –ø—É—Å—Ç–æ–π URL): {!r}',
                        index,
                        url
                        )
                    continue
                title = (data_el.get('title') or '').strip() or url
                description = (data_el.get('description') or '').strip()
                data.append({
                    'query': query,
                    'date': now,
                    'title': title,
                    'description': description,
                    'url': url
                })
                existing_urls.add(url_key)
                added += 1
        else:
            logger.warning(
                '–û–∂–∏–¥–∞–ª—Å—è —Å–ø–∏—Å–æ–∫ —Å—ã—Ä—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –ø–æ–ª—É—á–µ–Ω–æ: {!r}',
                type(results)
                )

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    logger.info(
        '–£—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ {} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π (–ø–æ—Å–ª–µ –¥–µ–¥—É–ø–∞). –í—Å–µ–≥–æ: {}',
        added,
        len(data)
        )
    return added
